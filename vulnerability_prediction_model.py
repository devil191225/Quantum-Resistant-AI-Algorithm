import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import pandas as pd
import time
import os
from datetime import datetime

np.random.seed(42)
tf.random.set_seed(42)

class VulnerabilityPredictionModel:
    def __init__(self, model_architecture="default"):
        self.model = None
        self.scaler = StandardScaler()
        self.history = None
        self.model_architecture = model_architecture
        self.training_time = 0
        self.trained = False
        self.checkpoint_dir = "model_checkpoints"
        if not os.path.exists(self.checkpoint_dir):
            os.makedirs(self.checkpoint_dir)
    
    def generate_synthetic_vulnerability_data(self, num_samples=1000, complexity_level="standard"):
        
        np.random.seed(42)
        print(f"Generating {num_samples} samples with {complexity_level} complexity...")
        
        data = {
            'n': np.random.choice([256, 512, 768, 1024], num_samples),
            'q': np.random.choice([2048, 3329, 4096, 7681, 8192], num_samples),
            'eta': np.random.choice([2, 3, 4, 5], num_samples),
            'k': np.random.choice([2, 3, 4], num_samples),
            'delta': np.random.uniform(1.0023, 1.0045, num_samples), 
            'implementation_noise': np.random.uniform(0, 0.3, num_samples)  
        }
        
        if complexity_level == "complex":
            data['compression_level'] = np.random.choice([1, 2, 3, 4], num_samples)
            data['error_correction'] = np.random.uniform(0.1, 0.9, num_samples)
            data['num_iterations'] = np.random.choice([1, 2, 3, 4, 5], num_samples)
        
        data['q_to_n_ratio'] = data['q'] / data['n']
        data['eta_to_q_ratio'] = data['eta'] / data['q']
        data['module_size'] = data['n'] * data['k']
        
        data['n_log2'] = np.log2(data['n'])
       
        lattice_vuln_base = 1.0 / (data['n'] * np.log2(data['q']))
        
        side_channel_vuln_base = 0.2 * data['implementation_noise'] * (1 + 0.5 * data['eta_to_q_ratio'])
       
        misuse_vuln_base = 0.1 * (data['k'] / 4) * (data['eta'] / 5)
        
        impl_vuln_base = 0.15 * data['implementation_noise'] * (1 - 0.5 * (data['n'] / 1024))

        quantum_vuln_base = 0.3 / (data['module_size'] * np.log2(data['q']))
        
        if complexity_level == "simple":
            noise_scale = 0.02
        elif complexity_level == "standard":
            noise_scale = 0.05
        else:  
            noise_scale = 0.08
            
            lattice_vuln_base += 0.1 * np.sin(data['n_log2'] * np.pi / 10)
            side_channel_vuln_base *= (1 + 0.2 * np.cos(data['q'] / 1000))
            quantum_vuln_base += 0.05 * np.tanh(data['eta_to_q_ratio'] * 10)
        
        noise = np.random.normal(0, noise_scale, num_samples)
        
        data['lattice_vulnerability'] = np.clip(lattice_vuln_base + np.random.normal(0, noise_scale, num_samples), 0, 1)
        data['side_channel_vulnerability'] = np.clip(side_channel_vuln_base + np.random.normal(0, noise_scale, num_samples), 0, 1)
        data['misuse_vulnerability'] = np.clip(misuse_vuln_base + np.random.normal(0, noise_scale * 0.5, num_samples), 0, 1)
        data['implementation_vulnerability'] = np.clip(impl_vuln_base + np.random.normal(0, noise_scale * 0.7, num_samples), 0, 1)
        data['quantum_vulnerability'] = np.clip(quantum_vuln_base + np.random.normal(0, noise_scale * 0.9, num_samples), 0, 1)
        
        data['overall_vulnerability'] = (
            0.3 * data['lattice_vulnerability'] +
            0.2 * data['side_channel_vulnerability'] +
            0.15 * data['misuse_vulnerability'] +
            0.15 * data['implementation_vulnerability'] +
            0.2 * data['quantum_vulnerability']
        )

        if complexity_level == "complex":
            unknown_factor = np.random.normal(0, 0.05, num_samples)
            data['overall_vulnerability'] = np.clip(data['overall_vulnerability'] + unknown_factor, 0, 1)
        
        
        if complexity_level != "simple":
            
            secure_configs = int(num_samples * 0.2)  
            
            secure_data = {
                'n': np.random.choice([768, 1024], secure_configs),
                'q': np.random.choice([4096, 7681, 8192], secure_configs),
                'eta': np.random.choice([2, 3], secure_configs),
                'k': np.random.choice([3, 4], secure_configs),
                'delta': np.random.uniform(1.0023, 1.0035, secure_configs),
                'implementation_noise': np.random.uniform(0, 0.1, secure_configs)
            }
            
            secure_data['q_to_n_ratio'] = secure_data['q'] / secure_data['n']
            secure_data['eta_to_q_ratio'] = secure_data['eta'] / secure_data['q']
            secure_data['module_size'] = secure_data['n'] * secure_data['k']
            secure_data['n_log2'] = np.log2(secure_data['n'])
            
            secure_data['lattice_vulnerability'] = np.clip(0.3 / (secure_data['n'] * np.log2(secure_data['q'])) + np.random.normal(0, 0.02, secure_configs), 0, 0.4)
            secure_data['side_channel_vulnerability'] = np.clip(0.1 * secure_data['implementation_noise'] + np.random.normal(0, 0.02, secure_configs), 0, 0.4)
            secure_data['misuse_vulnerability'] = np.clip(0.05 * (secure_data['k'] / 4) + np.random.normal(0, 0.02, secure_configs), 0, 0.4)
            secure_data['implementation_vulnerability'] = np.clip(0.1 * secure_data['implementation_noise'] + np.random.normal(0, 0.02, secure_configs), 0, 0.4)
            secure_data['quantum_vulnerability'] = np.clip(0.15 / (secure_data['module_size'] * np.log2(secure_data['q'])) + np.random.normal(0, 0.02, secure_configs), 0, 0.4)
            
            secure_data['overall_vulnerability'] = (
                0.3 * secure_data['lattice_vulnerability'] +
                0.2 * secure_data['side_channel_vulnerability'] +
                0.15 * secure_data['misuse_vulnerability'] +
                0.15 * secure_data['implementation_vulnerability'] +
                0.2 * secure_data['quantum_vulnerability']
            )
            
            if complexity_level == "complex":
                secure_data['compression_level'] = np.random.choice([1, 2], secure_configs)
                secure_data['error_correction'] = np.random.uniform(0.5, 0.9, secure_configs)
                secure_data['num_iterations'] = np.random.choice([3, 4, 5], secure_configs)
            
            for key in data:
                if key in secure_data:
                    data[key] = np.concatenate([data[key], secure_data[key]])
        
        return pd.DataFrame(data)
    
    def build_model(self, input_shape, architecture="default"):
        print(f"Building model with {architecture} architecture...")
        
        if isinstance(input_shape, int):
            input_shape = (input_shape,)
        
        if architecture == "deep":
            model = keras.Sequential([
                keras.layers.Input(shape=input_shape),
                keras.layers.Dense(64, activation='relu'),
                keras.layers.BatchNormalization(),
                keras.layers.Dropout(0.3),
                keras.layers.Dense(32, activation='relu'),
                keras.layers.BatchNormalization(),
                keras.layers.Dropout(0.3),
                keras.layers.Dense(16, activation='relu'),
                keras.layers.BatchNormalization(),
                keras.layers.Dropout(0.2),
                keras.layers.Dense(8, activation='relu'),
                keras.layers.BatchNormalization(),
                keras.layers.Dense(5, activation='sigmoid')
            ])
        elif architecture == "wide":
            model = keras.Sequential([
                keras.layers.Input(shape=input_shape),
                keras.layers.Dense(128, activation='relu'),
                keras.layers.BatchNormalization(),
                keras.layers.Dropout(0.4),
                keras.layers.Dense(64, activation='relu'),
                keras.layers.BatchNormalization(),
                keras.layers.Dropout(0.3),
                keras.layers.Dense(5, activation='sigmoid')
            ])
        else:  
            model = keras.Sequential([
                keras.layers.Input(shape=input_shape),
                keras.layers.Dense(32, activation='relu'),
                keras.layers.BatchNormalization(),
                keras.layers.Dropout(0.3),
                keras.layers.Dense(16, activation='relu'),
                keras.layers.BatchNormalization(),
                keras.layers.Dropout(0.2),
                keras.layers.Dense(5, activation='sigmoid')
            ])
        
        optimizer = keras.optimizers.Adam(learning_rate=0.001)
        
        model.compile(
            optimizer=optimizer,
            loss='mean_squared_error',
            metrics=['mae', 'mse']
        )
        
        return model
    
    def train(self, num_samples=2000, epochs=100, batch_size=32, validation_split=0.2, 
              data_complexity="standard", use_early_stopping=True, patience=15,
              cross_validation=False, n_folds=5):
        
        start_time = time.time()
        print(f"Starting model training with {num_samples} samples, {epochs} epochs, {batch_size} batch size")
        
        df = self.generate_synthetic_vulnerability_data(num_samples, data_complexity)
        
        features = [col for col in df.columns if col not in [
            'lattice_vulnerability', 'side_channel_vulnerability', 
            'misuse_vulnerability', 'implementation_vulnerability', 
            'quantum_vulnerability', 'overall_vulnerability'
        ]]
        
        targets = [
            'lattice_vulnerability', 'side_channel_vulnerability', 
            'misuse_vulnerability', 'implementation_vulnerability', 
            'quantum_vulnerability'
        ]
        
        print(f"Features used for training: {features}")
        print(f"Number of features: {len(features)}")
        
        X = df[features].values
        y = df[targets].values
        
        self.feature_names = features
        X_scaled = self.scaler.fit_transform(X)
        callbacks = []
        
        if use_early_stopping:
            early_stopping = keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=patience,
                restore_best_weights=True,
                verbose=1
            )
            callbacks.append(early_stopping)
        
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        checkpoint_path = os.path.join(self.checkpoint_dir, f"model_{timestamp}.keras")
        model_checkpoint = keras.callbacks.ModelCheckpoint(
            filepath=checkpoint_path,
            save_best_only=True,
            monitor='val_loss',
            verbose=1
        )
        callbacks.append(model_checkpoint)
        
        
        log_dir = os.path.join("logs", timestamp)
        tensorboard_callback = keras.callbacks.TensorBoard(
            log_dir=log_dir,
            histogram_freq=1
        )
        callbacks.append(tensorboard_callback)
        
        
        reduce_lr = keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=0.00001,
            verbose=1
        )
        callbacks.append(reduce_lr)
        
        if cross_validation:
            print(f"Performing {n_folds}-fold cross-validation...")
            kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)
            scores = []
            
            for fold, (train_idx, val_idx) in enumerate(kfold.split(X_scaled)):
                print(f"\nTraining fold {fold+1}/{n_folds}")
                X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]
                
                try:
                    
                    print(f"Feature dimension: {X_train.shape[1]}")
                    fold_model = self.build_model((X_train.shape[1],), self.model_architecture)
                    
                    
                    fold_history = fold_model.fit(
                        X_train, y_train,
                        epochs=epochs,
                        batch_size=batch_size,
                        validation_data=(X_val, y_val),
                        callbacks=callbacks,
                        verbose=1
                    )
                    
                    
                    val_loss, val_mae, val_mse = fold_model.evaluate(X_val, y_val, verbose=0)
                    scores.append(val_mae)
                    
                    
                    if fold == 0 or val_mae < min(scores[:-1]):
                        self.model = fold_model
                        self.history = fold_history
                        
                except Exception as e:
                    print(f"Error in fold {fold+1}: {str(e)}")
                    import traceback
                    traceback.print_exc()
                    if fold == 0:
                       
                        print("Falling back to standard training approach...")
                        cross_validation = False
                        break
            
            if cross_validation:  
                print(f"Cross-validation complete. Average MAE: {np.mean(scores):.4f}")
            
        else:
            X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=validation_split, random_state=42)
            
            print(f"Training model with {X_train.shape[0]} samples...")
            print(f"Feature dimension: {X_train.shape[1]}")
            self.model = self.build_model((X_train.shape[1],), self.model_architecture)
            
            self.history = self.model.fit(
                X_train, y_train,
                epochs=epochs,
                batch_size=batch_size,
                validation_split=validation_split,
                callbacks=callbacks,
                verbose=1
            )
            
            print("\nEvaluating model on test data...")
            test_loss, test_mae, test_mse = self.model.evaluate(X_test, y_test)
            print(f"Test MAE: {test_mae:.4f}")
            
            y_pred = self.model.predict(X_test)
            self._print_detailed_metrics(y_test, y_pred, targets)
        
        self.trained = True
        self.training_time = time.time() - start_time
        print(f"Training completed in {self.training_time:.2f} seconds")
        
        return self.history
    
    def _print_detailed_metrics(self, y_true, y_pred, target_names):
        print("\nDetailed Evaluation Metrics:")
        print(f"{'Vulnerability Type':<25} {'MAE':<10} {'RMSE':<10} {'RÂ²':<10}")
        print("-" * 55)
        
        for i, name in enumerate(target_names):
            mae = mean_absolute_error(y_true[:, i], y_pred[:, i])
            rmse = np.sqrt(mean_squared_error(y_true[:, i], y_pred[:, i]))
            r2 = r2_score(y_true[:, i], y_pred[:, i])
            
            print(f"{name:<25} {mae:<10.4f} {rmse:<10.4f} {r2:<10.4f}")
    
    def analyze_parameter_set(self, params):
       
        if not self.trained:
            raise ValueError("Model has not been trained yet. Call train() first.")
            
        required_params = ['n', 'q', 'eta', 'k']
        for param in required_params:
            if param not in params:
                raise ValueError(f"Missing required parameter: {param}")
        
        if 'delta' not in params:
            params['delta'] = 1.0025
        if 'implementation_noise' not in params:
            params['implementation_noise'] = 0.1
        
        params['q_to_n_ratio'] = params['q'] / params['n']
        params['eta_to_q_ratio'] = params['eta'] / params['q']
        params['module_size'] = params['n'] * params['k']
        params['n_log2'] = np.log2(params['n'])
        
        if not hasattr(self, 'feature_names'):
            feature_dict = {k: v for k, v in params.items() if k not in [
                'lattice_vulnerability', 'side_channel_vulnerability', 
                'misuse_vulnerability', 'implementation_vulnerability', 
                'quantum_vulnerability', 'overall_vulnerability'
            ]}
            df = pd.DataFrame([feature_dict])
        else:
            feature_dict = {}
            for feature in self.feature_names:
                if feature in params:
                    feature_dict[feature] = params[feature]
                else:
                    print(f"Warning: Feature '{feature}' not provided, using default value 0.")
                    feature_dict[feature] = 0.0
            df = pd.DataFrame([feature_dict])
        scaled_features = self.scaler.transform(df.values)
        predictions = self.model.predict(scaled_features)[0]
        vulnerabilities = {
            'lattice_vulnerability': float(predictions[0]),
            'side_channel_vulnerability': float(predictions[1]),
            'misuse_vulnerability': float(predictions[2]),
            'implementation_vulnerability': float(predictions[3]),
            'quantum_vulnerability': float(predictions[4]),
            'overall_vulnerability': float(np.mean(predictions))
        }
        vulnerabilities['confidence'] = self._estimate_prediction_confidence(df.values[0])
        
        return vulnerabilities
    
    def _estimate_prediction_confidence(self, features):
        sample_data = self.generate_synthetic_vulnerability_data(100).iloc[:, :len(features)]
        distances = []
        for _, row in sample_data.iterrows():
            dist = np.linalg.norm(row.values - features)
            distances.append(dist)
        
        min_distance = min(distances)
     
        confidence = np.exp(-min_distance / 5)  
        
        return min(1.0, max(0.0, confidence))
    
    def compare_configurations(self, configurations):
        if not self.trained:
            raise ValueError("Model has not been trained yet. Call train() first.")
            
        results = []
        
        for name, params in configurations.items():
            vulnerabilities = self.analyze_parameter_set(params)
            result = {'configuration': name}
            result.update(vulnerabilities)
            results.append(result)
        
        return pd.DataFrame(results)
    
    def plot_training_history(self):
        if self.history is None:
            print("Model hasn't been trained yet")
            return
            
        plt.figure(figsize=(15, 10))
        
        plt.subplot(2, 2, 1)
        plt.plot(self.history.history['loss'])
        plt.plot(self.history.history['val_loss'])
        plt.title('Model Loss')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'], loc='upper right')
        
        plt.subplot(2, 2, 2)
        plt.plot(self.history.history['mae'])
        plt.plot(self.history.history['val_mae'])
        plt.title('Mean Absolute Error')
        plt.ylabel('MAE')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'], loc='upper right')
        
        if 'mse' in self.history.history:
            plt.subplot(2, 2, 3)
            plt.plot(self.history.history['mse'])
            plt.plot(self.history.history['val_mse'])
            plt.title('Mean Squared Error')
            plt.ylabel('MSE')
            plt.xlabel('Epoch')
            plt.legend(['Train', 'Validation'], loc='upper right')
        
        if 'lr' in self.history.history:
            plt.subplot(2, 2, 4)
            plt.plot(self.history.history['lr'])
            plt.title('Learning Rate')
            plt.ylabel('Learning Rate')
            plt.xlabel('Epoch')
            plt.yscale('log')
        
        plt.tight_layout()
        plt.suptitle(f"Training Performance ({self.model_architecture} architecture)", y=1.02, fontsize=16)
        plt.show()
    
    def plot_vulnerability_comparison(self, comparison_df):
       
        categories = ['lattice_vulnerability', 'side_channel_vulnerability', 
                     'misuse_vulnerability', 'implementation_vulnerability', 'quantum_vulnerability']
        category_names = ['Lattice', 'Side-Channel', 'Misuse', 'Implementation', 'Quantum']
        
        configurations = comparison_df['configuration'].tolist()
        data = comparison_df[categories].values
        
        plt.figure(figsize=(14, 8))
        
        x = np.arange(len(category_names))
        width = 0.8 / len(configurations) 
        
        colors = plt.cm.viridis(np.linspace(0, 1, len(configurations)))
        
        for i, config in enumerate(configurations):
            pos = x - 0.4 + (i + 0.5) * width
            bars = plt.bar(pos, data[i], width=width, label=config, color=colors[i])
            
            for bar in bars:
                height = bar.get_height()
                plt.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                        f'{height:.2f}',
                        ha='center', va='bottom', fontsize=8, rotation=45)
        
        plt.ylabel('Vulnerability Score (Lower is Better)')
        plt.title('Vulnerability Comparison Across Configurations', fontsize=14)
        plt.xticks(x, category_names, fontsize=12)
        plt.legend(title="Configuration", bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.ylim(0, min(1.0, max(np.max(data) + 0.1, 0.5)))
        plt.axhline(y=0.5, linestyle='--', color='r', alpha=0.3)
        plt.text(x[-1] + 0.5, 0.5, 'Critical Threshold', va='center', alpha=0.7, fontsize=10)
        plt.grid(axis='y', linestyle='--', alpha=0.3)    
        plt.tight_layout()
        plt.show()
        self.plot_radar_comparison(comparison_df)
        self.plot_overall_comparison(comparison_df)
    
    def plot_radar_comparison(self, comparison_df):
        categories = ['lattice_vulnerability', 'side_channel_vulnerability', 
                     'misuse_vulnerability', 'implementation_vulnerability', 'quantum_vulnerability']
        category_names = ['Lattice\nAttacks', 'Side-Channel\nAttacks', 'Misuse\nVulnerability', 
                         'Implementation\nFlaws', 'Quantum\nThreats']

        plt.figure(figsize=(12, 10))
        ax = plt.subplot(111, polar=True)
        N = len(categories)
        angles = [n / float(N) * 2 * np.pi for n in range(N)]
        angles += angles[:1] 
        colors = plt.cm.viridis(np.linspace(0, 1, len(comparison_df)))
        for i, row in comparison_df.iterrows():
            values = [row[cat] for cat in categories]
            values = [1 - v for v in values]
            values += values[:1]
            ax.plot(angles, values, 'o-', linewidth=2, label=row['configuration'], color=colors[i])
            ax.fill(angles, values, alpha=0.1, color=colors[i])
        
        
        plt.xticks(angles[:-1], category_names, fontsize=12)
        
        plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
        
        
        ax.set_ylim(0, 1)
        
        
        ax.set_rticks([0.25, 0.5, 0.75, 1])  
        ax.set_yticklabels(['75%', '50%', '25%', '0%'], fontsize=10)  
        ax.grid(True, alpha=0.3)
        
        plt.title('Vulnerability Comparison (Higher is Better)', size=15)
        
        plt.tight_layout()
        plt.show()
    
    def plot_overall_comparison(self, comparison_df):

        plt.figure(figsize=(12, 6))
        sorted_df = comparison_df.sort_values('overall_vulnerability')
        configs = sorted_df['configuration'].tolist()
        vulnerabilities = sorted_df['overall_vulnerability'].tolist()
        has_confidence = 'confidence' in sorted_df.columns
        colors = []
        for vuln in vulnerabilities:
            if vuln < 0.25:
                colors.append('green')
            elif vuln < 0.5:
                colors.append('yellowgreen')
            elif vuln < 0.75:
                colors.append('orange')
            else:
                colors.append('red')
        
        bars = plt.barh(configs, vulnerabilities, color=colors, alpha=0.7)
        
        
        if has_confidence:
            confidences = sorted_df['confidence'].tolist()
            for i, (config, vuln, conf) in enumerate(zip(configs, vulnerabilities, confidences)):
                
                error = (1 - conf) * 0.1  
                plt.errorbar(vuln, i, xerr=error, fmt='o', color='black', capsize=5)
                
                plt.text(vuln + 0.02, i, f"Confidence: {conf:.1%}", va='center')
        
        plt.axvline(x=0.25, linestyle='--', color='green', alpha=0.5)
        plt.axvline(x=0.5, linestyle='--', color='orange', alpha=0.5)
        plt.axvline(x=0.75, linestyle='--', color='red', alpha=0.5)
        
        plt.text(0.25, -0.5, 'Low', ha='center', color='green')
        plt.text(0.5, -0.5, 'Medium', ha='center', color='orange')
        plt.text(0.75, -0.5, 'High', ha='center', color='red')
        
        for i, v in enumerate(vulnerabilities):
            plt.text(v + 0.01, i, f"{v:.3f}", va='center')
        
        plt.xlabel('Overall Vulnerability Score (Lower is Better)')
        plt.title('Overall Vulnerability Comparison', fontsize=14)
        
       
        plt.xlim(0, min(1.0, max(vulnerabilities) + 0.2))
        
        plt.grid(axis='x', linestyle='--', alpha=0.3)
        plt.tight_layout()
        plt.show()
    
    def parameter_sensitivity_analysis(self, base_params, param_ranges=None):
        
        if not self.trained:
            raise ValueError("Model has not been trained yet. Call train() first.")
        
        if param_ranges is None:
            param_ranges = {
                'n': [256, 512, 768, 1024],
                'q': [2048, 3329, 4096, 7681, 8192],
                'eta': [2, 3, 4, 5],
                'k': [2, 3, 4]
            }
        
        results = []
        
        base_vulnerabilities = self.analyze_parameter_set(base_params)
        base_result = {'parameter': 'Base', 'value': 'Base', 'configuration': 'Base'}
        base_result.update({k: v for k, v in base_vulnerabilities.items() if k != 'confidence'})
        results.append(base_result)
        
        for param, values in param_ranges.items():
            for value in values:
                if param in base_params and base_params[param] == value:
                    continue
                
                modified_params = base_params.copy()
                modified_params[param] = value
                
                vulnerabilities = self.analyze_parameter_set(modified_params)
                
                
                result = {
                    'parameter': param,
                    'value': value,
                    'configuration': f"{param}={value}"
                }
                result.update({k: v for k, v in vulnerabilities.items() if k != 'confidence'})
                results.append(result)
        
        return pd.DataFrame(results)
    
    def plot_sensitivity_analysis(self, sensitivity_df):
       
        params = sensitivity_df['parameter'].unique()
        base_row = sensitivity_df[sensitivity_df['parameter'] == 'Base'].iloc[0]
        
        
        n_params = len(params) - 1  
        fig, axes = plt.subplots(n_params, 1, figsize=(12, 4*n_params))
        
        if n_params == 1:
            axes = [axes]  
        
        param_idx = 0
        for param in params:
            if param == 'Base':
                continue
                
            ax = axes[param_idx]
            param_df = sensitivity_df[sensitivity_df['parameter'] == param]
            
            param_df = param_df.sort_values('value')
            
            values = param_df['value'].tolist()
            overall_vuln = param_df['overall_vulnerability'].tolist()
            
            ax.plot(values, overall_vuln, 'o-', color='red', linewidth=2, label='Overall')
            
            for vuln_type in ['lattice_vulnerability', 'side_channel_vulnerability', 
                              'misuse_vulnerability', 'implementation_vulnerability', 'quantum_vulnerability']:
                vuln_values = param_df[vuln_type].tolist()
                short_name = vuln_type.split('_')[0].capitalize()
                ax.plot(values, vuln_values, 'o--', linewidth=1, alpha=0.7, label=short_name)
            
            ax.axhline(y=base_row['overall_vulnerability'], linestyle='--', color='black', alpha=0.5, 
                      label=f'Base Overall ({base_row["overall_vulnerability"]:.3f})')
            
            ax.set_xlabel(f'Parameter Value: {param}')
            ax.set_ylabel('Vulnerability Score')
            ax.set_title(f'Sensitivity Analysis for Parameter: {param}')
            ax.grid(True, alpha=0.3)
            ax.legend(loc='best')
            
            min_idx = overall_vuln.index(min(overall_vuln))
            min_value = values[min_idx]
            min_vuln = overall_vuln[min_idx]
            ax.scatter([min_value], [min_vuln], s=100, color='green', zorder=5)
            ax.text(min_value, min_vuln+0.02, f'Optimum: {min_value} ({min_vuln:.3f})', 
                   ha='center', va='bottom', fontweight='bold')
            
            param_idx += 1
        
        plt.tight_layout()
        plt.show()
    
    def save_model(self, filepath=None):
        if not self.trained:
            raise ValueError("Model has not been trained yet. Call train() first.")
            
        if filepath is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = f"vulnerability_model_{timestamp}"
        
        model_path = f"{filepath}.keras"
        self.model.save(model_path)
        
        import joblib
        scaler_path = f"{filepath}_scaler.pkl"
        joblib.dump(self.scaler, scaler_path)
        
        print(f"Model saved to {model_path}")
        print(f"Scaler saved to {scaler_path}")
        
        return model_path, scaler_path
    
    def load_model(self, model_path, scaler_path):
        self.model = keras.models.load_model(model_path)
        
        import joblib
        self.scaler = joblib.load(scaler_path)
        
        self.trained = True
        
        print(f"Model loaded from {model_path}")
        print(f"Scaler loaded from {scaler_path}")
        
        return self
    
    def generate_optimal_configuration(self, target_security_level=192, optimization_iterations=50):
        if not self.trained:
            raise ValueError("Model has not been trained yet. Call train() first.")
            
        print(f"Generating optimal configuration for {target_security_level}-bit security level...")
        
        
        param_ranges = {
            'n': [256, 512, 768, 1024],
            'q': [2048, 3329, 4096, 7681, 8192],
            'eta': [2, 3, 4, 5],
            'k': [2, 3, 4]
        }
        
        if target_security_level <= 128:
            best_params = {'n': 512, 'q': 3329, 'eta': 2, 'k': 2}  
        elif target_security_level <= 192:
            best_params = {'n': 768, 'q': 3329, 'eta': 2, 'k': 3}  
        else:
            best_params = {'n': 1024, 'q': 3329, 'eta': 2, 'k': 4}  
        best_params['implementation_noise'] = 0.05
        security_bits = min(256, 0.4 * best_params['n'] * best_params['k'] * np.log2(best_params['q'] / best_params['eta']))
        
        best_vulnerabilities = self.analyze_parameter_set(best_params)
        best_score = best_vulnerabilities['overall_vulnerability']
        
        print(f"Initial configuration: {best_params}")
        print(f"Initial security level: {security_bits:.1f} bits")
        print(f"Initial vulnerability score: {best_score:.4f}")
        
        for i in range(optimization_iterations):
            test_params = best_params.copy()
            param = np.random.choice(list(param_ranges.keys()))
            current_value = test_params[param]
            available_values = [v for v in param_ranges[param] if v != current_value]
            
            if not available_values:
                continue  
                
            test_params[param] = np.random.choice(available_values)
            
            security_bits = min(256, 0.4 * test_params['n'] * test_params['k'] * np.log2(test_params['q'] / test_params['eta']))
            
            if security_bits < target_security_level:
                continue
            
            test_vulnerabilities = self.analyze_parameter_set(test_params)
            test_score = test_vulnerabilities['overall_vulnerability']
            
            if test_score < best_score:
                best_params = test_params
                best_vulnerabilities = test_vulnerabilities
                best_score = test_score
                
                print(f"Iteration {i+1}: Found better configuration")
                print(f"  Parameters: {best_params}")
                print(f"  Security level: {security_bits:.1f} bits")
                print(f"  Vulnerability score: {best_score:.4f}")
        
        final_security = min(256, 0.4 * best_params['n'] * best_params['k'] * np.log2(best_params['q'] / best_params['eta']))
        
        print("\nOptimization complete!")
        print(f"Final optimal configuration: {best_params}")
        print(f"Security level: {final_security:.1f} bits")
        print(f"Overall vulnerability score: {best_score:.4f}")
        
        result = {
            'parameters': best_params,
            'security_level': final_security,
            'vulnerabilities': best_vulnerabilities
        }
        
        return result


if __name__ == "__main__":
    model = VulnerabilityPredictionModel(model_architecture="deep")
    
    model.train(
        num_samples=5000,
        epochs=200,
        batch_size=64,
        data_complexity="complex",
        use_early_stopping=True,
        patience=20,
        cross_validation=True,
        n_folds=5
    )
    
    model.plot_training_history()
    
    standard_configs = {
        'Kyber-512': {'n': 512, 'q': 3329, 'eta': 2, 'k': 2},
        'Kyber-768': {'n': 768, 'q': 3329, 'eta': 2, 'k': 3},
        'Kyber-1024': {'n': 1024, 'q': 3329, 'eta': 2, 'k': 4}
    }
    
    optimal_config = model.generate_optimal_configuration(target_security_level=192)
    standard_configs['AI-Optimized'] = optimal_config['parameters']
    comparison = model.compare_configurations(standard_configs)
    print("\nVulnerability Analysis Results:")
    print(comparison)
    model.plot_vulnerability_comparison(comparison)
    sensitivity_df = model.parameter_sensitivity_analysis(optimal_config['parameters'])
    model.plot_sensitivity_analysis(sensitivity_df)
    model.save_model("optimized_vulnerability_model")